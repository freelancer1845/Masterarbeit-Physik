\documentclass[master.tex]{subfiles}
 
\begin{document}


\lstset { %
    language=C++,
    backgroundcolor=\color{black!5}, % set backgroundcolor
    basicstyle=\footnotesize,% basic font setting
    commentstyle=\color{green}, % comment color
    keywordstyle=\color{blue}, % keyword color
    stringstyle=\color{red} % string color
}

\subsection{For Loops}
A common pattern in the simulation is to iterate over all the grid points.
This is done using three for-Loops.
\begin{lstlisting}
for (int z = domain.g_z; z < domain.n_z + domain.g_z; z++) {
  for (int x = domain.g_x; x < domain.n_x + domain.g_x; x++) {
    for (int y = domain.g_y; y < domain.n_y + domain.g_y; y++) {
       ...
    }
  }
}
\end{lstlisting}
Usually the outer loop only has a few iterations $\sim 8$ and the inner ones have 100-1000 iterations.
If there are no data dependencies between iterations the most inner loop can be vectorized by the compiler quite easily. This is a simple straight forward optimization where one usually swaps memory space (which is abundant in our case) for computation speed. For example if we wanted to approximate the derivative using a forward difference scheme in a loop of this form
\begin{lstlisting}
for (int x = 0; x < n_x - 1; x++) {
  array[x] = (array[x + 1] - array[x]) / h_x    
}
\end{lstlisting}
where the memory is reused, the compiler won't be able to use vectorization. Whereas if we save the result in a second array this would not be a problem for the compiler.
\subsection{OpenMP} \label{sec:open-mp-method}
Another simple way to parallelize such loops is via OpenMP and its \textit{worksharing constructs}. This mainly applies to the outer loop since worksharing to threads is only effective if the shared work is heavy enough. There are different options available. The most straight forward is the following:
\begin{lstlisting}
#pragma omp parallel for
for (int z = domain.g_z; z < domain.n_z + domain.g_z; z++) {
  for (int x = domain.g_x; x < domain.n_x + domain.g_x; x++) {
    for (int y = domain.g_y; y < domain.n_y + domain.g_y; y++) {
       ...
    }
  }
}
\end{lstlisting}
\begin{blockquote}
  \small
  Sometimes it might even be worth to use OpenMP to parallelize the loop over the x-coordinate especially if the resolution in y-direction is large enough (\textgreater 1000 grid points).
\end{blockquote}

But this poses a problem since now a parallel region is created for every such loop. If $n_z=8$ and there are 8 threads available on the system this is not a big problem. But newer CPU's easily have 16, 32, 64 threads available. Thus parallelizing with OpenMP in this way cannot use all of the available threads. Another option is the newer (OpenMP Specification 4.5) \textit{taskloop} pragma. This is very useful if we have for example four such loops with $n_z=8$ that do not depend on each other \textit{(loop1, loop2, loop3, loop4)}. Each loop is constructed in the following way:
\begin{lstlisting}
#pragma omp taskloop
for (int z = domain.g_z; z < domain.n_z + domain.g_z; z++) {
  for (int x = domain.g_x; x < domain.n_x + domain.g_x; x++) {
    for (int y = domain.g_y; y < domain.n_y + domain.g_y; y++) {
        ...
    }
  }
}
\end{lstlisting}


To achieve good parallelization the call structure of theses loops is done in the following way:
\begin{lstlisting}
#pragma omp parallel sections
{
  #pragma omp section 
    { loop1(); }
  #pragma omp section
    { loop2(); }
  #pragma omp section
    { loop3(); }
  #pragma omp section
    { loop4(); }
}
\end{lstlisting}
Now each section uses at first only one thread. But when execution reaches the taskloop construct further threads are \textit{borrowed}. In this case the maximum used threads could be 32 if each iteration of the loops is executed on a single thread.\newline

\begin{blockquote}
  \small
  It is necessary to use the \textit{taskloop} construct here because the for loop construct requires all threads of the parallel region to reach the for loop which is not given anymore if \textit{sections} are used.
\end{blockquote}

The code is structured such that theses parallelizations are easily employed. Therefore it was sometimes necessary to use more memory than was absolutely necessary (for instance to store intermediate results).\newline
There are two levels of grid parallelization employed throughout the code. The first level parallelizes in Z direction and the second in X direction. This is handled using a simple macro definition. Every OpenMP parallelized loop has the following macro \textit{guard}:

\begin{lstlisting}
#ifdef PARALLELIZE_Z
#pragma omp taskloop
#endif
for (...)
\end{lstlisting}
This allows for smarter debugging and selective parallelization levels.

\subsection{GPU Offloading using CUDA}
To further optimize the code one can move some of the calculations to the graphics card. The graphics processing unit is optimized for massive parallelism and for example the SOR-Solver can be implemented effectively. The downside to this optimization is that the code becomes very complex. Two main factors play a role. The first one is that we lose unified memory. This requires us to spend some work on transferring data from main memory to the GPU memory and back, cluttering the code and making it hard to understand for someone not familiar with CUDA. The second key factor is that the code on the GPU is not sequential anymore. It is executed in parallel by many threads which makes it difficult to write readable code.\newline
Here it is briefly described what is necessary to move the SOR solver effectively to the GPU:
\subsubsection{Interface}
At first one has to decide where the GPU implementation interacts with the CPU code. In this case the \ac{SOR}-solver is modeled using an object which later on has a single member function: \textit{void execute(N, F, U)} where the types are omitted, but \textit{N, F and U} are $n_x \times n_y$ matrices including ghost cells. They represent $N$, $\rho$ and $\Psi$ of \autoref{eq:general-poisson}. Since we want to implement this solver on the GPU the interface of the GPU solver should look the same. If so we can simply call the GPU solver in the CPU implementation of the solver or replace the code where the CPU implementation is called.
\subsubsection{Memory Allocation}
Since the interface is now decided on we need to allocate memory on the GPU device (see \autoref{sec:multi-gpu} for multiple GPUs) that represent \textit{N, F, U}. Now on every call to the solver the host state needs to be transferred to the device. This of course creates an overhead that could be avoided if all data is on the GPU but this overhead can be minimized. In theory a GPU device can transfer memory from host to device while executing code. This means if two solvers run on a single device, during the execution of the first solver, the data necessary for the second solver can be transferred in parallel. But this is only possible if the host memory is \textit{page-locked} (which is an restriction on how the host is allowed to handle the allocated memory). So not only is it necessary to allocate memory on the GPU but one should also make sure that \textit{N, F, U} are page-locked. The CUDA framework has functions that allocate page-locked memory on the host system.
\subsubsection{Execution}
The first step is to transfer the input data to the GPU device as is described in the previous section. Afterwards the execution starts.\newline
Code that runs on the GPU is written into functions called kernels. A kernel is executed by multiple threads in parallel (for example 1000). If all of these threads do meaningful work this can dramatically increase performance just by this massive parallelism. But another strong feature is the high memory bandwidth one can achieve on a GPU. The \ac{SOR}-solver is a memory bound problem so the algorithm should access memory in specific patterns that exploit the given hardware structure. Specific implementation details are out of scope of this thesis but the implementation uses a memory access pattern that utilizes \textit{shared memory} and \textit{wrap}-level coalesced memory reading (which reduces global memory access and by that increases performance).\newline
After the necessary kernels are executed the result is transferred back to host memory (this transfer can again overlap with kernel executions from other solvers) and the solver is done and ready for the next input data.\newline
From this structure it can easily be seen that by having multiple solver objects higher order parallelism can easily be achieved.


\subsection{GPU Code vs CPU Code}
Since GPU code is sometimes hard to follow a comparable CPU implementation is always present allowing to verify the correctness.\newline
Another important feature is to make it possible to compile the code without Cuda available since not every workstation is Cuda capable. In this simulation code all the Cuda code is kept into a single CMake target that is compiled into a static library by the Cuda compiler. It is then later linked to the rest of the simulation. The big advantage is that only the Cuda dependent code needs to be compiled with the Cuda compiler which for example not always supports the newest standard. If theres no Cuda compiler available the CMake target is simply not compiled and the simulation code may not call any functions that in turn call functions from the Cuda compiled CMake target since this would result in a linking error. This is implemented by a single macro definition \textit{(COMPILE\_CUDA\_CODE)}.

\subsection{Multi GPU Systems}\label{sec:multi-gpu}
To make full use of all available GPUs on a given system the solvers need to be distributed over the available devices. This is done via a simple static variable that keeps track on what device the last solver was created on. So if one creates 8 \ac{SOR} solvers on a system with 4 devices the first solver is on device 0 the second on device 1 and so forth. The fifth solver is again created on device 0. This is not always an ideal distribution. For example if the first solver and the fifth solver always take longer for some reason, the other three devices are underutilized. But it is a simple construct to implement which may be changed later on.\newline
To restrict the use of a single GPU device by multiple threads semaphores are used. The concurrent threads per device can be controlled by a macro definition \textit{(THREADS\_PER\_DEVICE - defaults to 2)}. It is useful to have multiple threads execute on a single device to increase GPU utilization since then memory transfers can overlap with kernel executions. If a solver is executed it first tries to acquire a permit from the semaphore associated to the device. If non is available the thread is blocked. After the execution of the solver the permit is released and waiting threads are notified.

\subsection{Windows compilation}
The simulation code can be compiled on Windows systems using MinGW or any other GCC port to Windows. But for now CUDA only supports compilation on Windows through the Microsoft C++ (MSVC) compiler thus the CUDA code cannot be compiled with GCC on Windows. But it should be possible to compile the CUDA library with MSVC and link against it in a later stage though this has not been tested or set up.


\end{document}