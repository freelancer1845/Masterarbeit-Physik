\documentclass[master.tex]{subfiles}
 
\begin{document}


\subsection{For Loops}
A common pattern in the simulation is to iterate over all the grid points.
This is done using three for-Loops.
\begin{lstlisting}
for (int z = domain.g_z; z < domain.n_z + domain.g_z; z++) {
  for (int x = domain.g_x; x < domain.n_x + domain.g_x; x++) {
    for (int y = domain.g_y; y < domain.n_y + domain.g_y; y++) {
       ...
    }
  }
}
\end{lstlisting}
Usually the outer loops only has a few iterations $\sim 8$ and the inner ones have 100-1000 iterations.
If there are no data dependencies between iterations the most inner loop can be vectorized by the compiler quite easily. This is a simple straight forward optimization where one usually swaps memory space (which is abundant in our case) against computation speed. For example if we wanted to approximate the derivative using a forward difference scheme a loop of this form
\begin{lstlisting}
for (int x = 0; x < n_x - 1; x++) {
  array[x] = (array[x + 1] - array[x]) / h_x    
}
\end{lstlisting}
where the memory is reused, the compiler won't be able to use vectorization. Whereas if we save the result in a second array this would not be a problem for the compiler.
\subsection{OpenMP} \label{sec:open-mp-method}
Another simple way to parallelize such loops is via OpenMP and its \textit{worksharing constructs}. This mainly applies to the outer loop since worksharing to threads is only effective if the shared work is heavy enough. There are different options available. The most straight forward is the following:
\begin{lstlisting}
#pragma omp parallel for
for (int z = domain.g_z; z < domain.n_z + domain.g_z; z++) {
  for (int x = domain.g_x; x < domain.n_x + domain.g_x; x++) {
    for (int y = domain.g_y; y < domain.n_y + domain.g_y; y++) {
       ...
    }
  }
}
\end{lstlisting}
But this poses a problem since now a parallel region is created for every such loop. If $n_z=8$ and there are 8 threads available on the system this is not a big problem. But newer CPU's easily have 16, 32, 64 threads available. Thus parallelizing with OpenMP in this way cannot use all of the available threads. Another option is the newer (OpenMP Specification 4.5) \textit{taskloop} pragma. This is very useful if we have for example four such loops with $n_z=8$ that do not depend on each other \textit{(loop1, loop2, loop3, loop4)}. Each loop is constructed in the following way:
\begin{lstlisting}
  #pragma omp taskloop
  for (int z = domain.g_z; z < domain.n_z + domain.g_z; z++) {
    for (int x = domain.g_x; x < domain.n_x + domain.g_x; x++) {
      for (int y = domain.g_y; y < domain.n_y + domain.g_y; y++) {
         ...
      }
    }
  }
\end{lstlisting}
\begin{blockquote}
  \small
  It is necessary to use the \textit{taskloop} construct here because the for loop construct requires all threads of the parallel region to reach the for loop which is not given anymore if \textit{sections} are used.
\end{blockquote}

To achieve good parallelization the call structure of theses loops is done in the following way:
\begin{lstlisting}
  #pragma omp parallel sections
  {
    #pragma omp section 
      { loop1(); }
    #pragma omp section
      { loop2(); }
    #pragma omp section
      { loop3(); }
    #pragma omp section
      { loop4(); }
  }
\end{lstlisting}
Now each section uses at first only one thread. But when execution reaches the taskloop construct further threads are \textit{borrowed}. In this case the maximum used threads could be 32 if each iteration of the loops is executed on a single thread.\newline

\begin{blockquote}
  \small
  Sometimes it might even be worth to use OpenMP to parallelize the loop over the x-coordinate especially if the resolution in y-direction is large enough (\textgreater 1000 grid points).
\end{blockquote}
The code is structured such that this parallelization is easily employed. Therefore it was sometimes necessary to use more memory than was absolutely necessary (for instance to store intermediate results).\newline
There are two levels of grid parallelization employed throughout the code. The first level parallelizes in Z direction and the second in X direction. This is handled using a simple macro definition. Every OpenMP parallelized loop has the following macro \textit{guard}:

\begin{lstlisting}
  #ifdef PARALLELIZE_Z
  #pragma omp taskloop
  #endif
  for (...)
\end{lstlisting}
This allows for smarter debugging and selective parallelization levels.

\subsection{GPU Offloading using CUDA}
To further optimize the code one can move some of the calculation to the graphics card. The graphics processing unit is optimized for massive parallelism and for example the SOR-Solver can be implemented effectively. The downside to this optimization is that the code becomes very complex. Two main factors play a role. The first one is that we lose unified memory. This requires us to spend some work on transferring data from main memory to the GPU memory and back, cluttering the code and making it hard to understand for someone not familiar with CUDA. The second key factor is that the code on the GPU is not sequential anymore. It is executed in parallel by many threads which makes it difficult to write readable code.\newline

\subsection{GPU Code vs CPU Code}
Since GPU code is sometimes hard to follow a comparable CPU implementation is always be present allowing to verify the correctness.\newline
Another important feature is to make it possible to compile the code without Cuda available since not every workstation is Cuda capable. In this simulation code all the Cuda code is kept into a single CMake target that is compiled into a static library by the Cuda compiler. It is then later linked to the rest of the simulation. The big advantage is that only the Cuda dependent code needs to be compiled with the Cuda compiler which for example not always supports the newest standard. If theres no Cuda compiler available the CMake target is simply not compiled and the simulation code may not call any functions that in turn call functions from the Cuda compiled CMake target since this would result in a linking error. This is implemented by a single macro definition \textit{(COMPILE\_CUDA\_CODE)}.

\subsection{Multi GPU Systems}
To make full use of all available GPUs on a given system the solvers need to be distributed over the available devices. This is done via a simple static variable that keeps track on what device the last solver was created on. So if one creates 8 \ac{SOR} solvers on a system with 4 devices the first solver is on device 0 the second on device 1 and so forth. The fifth solver is again created on the device 0. This is not always an ideal distribution. For example if the first solver and the fifth solver always take longer for some reason, the other three devices are underutilized. But it is a simple construct to implement which may be changed later on.\newline
To restrict the use of a single GPU device by multiple threads semaphores are used. The concurrent threads per device can be controlled by a macro definition \textit{(THREADS\_PER\_DEVICE - defaults to 2)}. It is useful to have multiple threads execute on a single device to increase GPU utilization since then memory transfers can overlap with kernel executions. If a solver is executed it first tries to acquire a permit from the semaphore associated to the device. If non is available the thread is blocked. After the execution of the solver the permit is released and waiting threads are notified.

\subsection{Windows compilation}
The simulation code can be compiled on Windows systems using MinGW or any other GCC port to Windows. But for now CUDA only supports compilation on Windows through the Microsoft C++ (MSVC) compiler thus the CUDA code cannot be compiled with GCC on Windows. But it should be possible to compile the CUDA library with MSVC and link against it in a later stage though this has not been tested or set up.


\end{document}