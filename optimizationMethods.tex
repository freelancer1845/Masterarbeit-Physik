\documentclass[master.tex]{subfiles}
 
\begin{document}


\subsection{For Loops}
A common pattern in the simulation is to iterate over all the grid points.
This is done using three for-Loops.
\begin{lstlisting}
for (int z = domain.g_z; z < domain.n_z + domain.g_z; z++) {
  for (int x = domain.g_x; x < domain.n_x + domain.g_x; x++) {
    for (int y = domain.g_y; y < domain.n_y + domain.g_y; y++) {
       ...
    }
  }
}
\end{lstlisting}
Usually the outer loops only has a few iterations $\sim 8$ and the inner ones have 100-1000 iterations.
If there are no data dependencies between iterations the most inner loop can be vectorzied by the compiler quite easily. This is a simple straight forward optimization where one usually swaps memory space (which is abundant in our case) against computation speed. For example if we wanted to approximate the derivative using a forward difference scheme a loop of this form
\begin{lstlisting}
for (int x = 0; x < n_x - 1; x++) {
  array[x] = (array[x + 1] - array[x]) / h_x    
}
\end{lstlisting}
where the memory is reused, the compiler won't be able to vectorize. Whereas if we save the result in a second array this would not be a problem for the compiler.
\subsection{OpenMP} \label{sec:open-mp-method}
Another simple way to parallelize such loops is via OpenMP and its \textit{worksharing constructs}. This mainly applies to the outer loop since worksharing to threads is only effective if the shared work is heavy enough. There are different options available. The most straight forward is the following:
\begin{lstlisting}
#pragma omp parallel for
for (int z = domain.g_z; z < domain.n_z + domain.g_z; z++) {
  for (int x = domain.g_x; x < domain.n_x + domain.g_x; x++) {
    for (int y = domain.g_y; y < domain.n_y + domain.g_y; y++) {
       ...
    }
  }
}
\end{lstlisting}
But this poses a problem since now a parallel region is created for every such loop. If $n_z=8$ and there are 8 threads available on the system this is not a big problem. But newer CPU's easily have 16, 32, 64 threads available. Thus parallelizing with OpenMP in this way cannot use all of the available threads. Another option is the newer (OpenMP Specification 4.5) \textit{taskloop} pragma. This is very useful if we have for example four such loops with $n_z=8$ that do not depend on each other \textit{(loop1, loop2, loop3, loop4)}. Each loop is constructed in the following way:
\begin{lstlisting}
  #pragma omp taskloop
  for (int z = domain.g_z; z < domain.n_z + domain.g_z; z++) {
    for (int x = domain.g_x; x < domain.n_x + domain.g_x; x++) {
      for (int y = domain.g_y; y < domain.n_y + domain.g_y; y++) {
         ...
      }
    }
  }
\end{lstlisting}
\begin{blockquote}
  \small
  It is necessary to use the \textit{taskloop} construct here because the for loop construct requires all threads of the parallel region to reach the for loop which is not given anymore if \textit{sections} are used.
\end{blockquote}

To achieve good parallelization the call structure of theses loops is done in the following way:
\begin{lstlisting}
  #pragma omp parallel sections
  {
    #pragma omp section 
      { loop1(); }
    #pragma omp section
      { loop2(); }
    #pragma omp section
      { loop3(); }
    #pragma omp section
      { loop4(); }
  }
\end{lstlisting}
Now each section uses at first only one thread. But when execution reaches the taskloop construct further threads are \textit{borrowed}. In this case the totally used threads could be 32 if each iteration of the loops is executed on a single thread.\newline



Sometimes it might even be worth to use OpenMP to parallelize the loop over the x-coordinate especially if the resolution in y-direction is large enough (\textgreater 1000 grid points).

\subsection{GPU Offloading using CUDA}
To further optimize the code one can move some of the calculation to the graphics card. The graphics processing unit is optimized for massive parallelism and for example the SOR-Solver can be implemented effectively. The downside to this optimization is that the code becomes very complex. Two main factors play a role. The first one is that we lose unified memory. This requires us to spend some work on transferring data from main memory to the GPU memory and back, cluttering the code and making it hard to understand for someone not familiar with CUDA. The second key factor is that the code on the GPU is not sequential anymore. It is executed in parallel by many threads which makes it difficult to write readable code.\newline
Nevertheless it can be very effective as will be shown later.


\end{document}